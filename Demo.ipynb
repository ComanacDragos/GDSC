{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "tu1q-TzAaECV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as K\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JnnE1m7FZ9Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numpy\n",
        "\n"
      ],
      "metadata": {
        "id": "DJz3CmpKI1Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[1, 2, 3]])\n",
        "\n",
        "print(a, a.shape)\n",
        "print(a.T, a.T.shape)"
      ],
      "metadata": {
        "id": "Lbj3KaaOJHAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a[:, 1])\n",
        "print(a[:, 1:])\n",
        "print(a[..., ::2])\n",
        "print(a>1)\n",
        "print(a[a>1])\n",
        "print(a[:, [0, 2]])"
      ],
      "metadata": {
        "id": "Mu0QbdcOVb5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = np.array([[3, 2, 1]])\n",
        "\n",
        "print(\"a+b\", a+b)\n",
        "print(\"a*b\", a*b)\n",
        "print(\"a dot b\", np.dot(a, b.T))"
      ],
      "metadata": {
        "id": "_EN8iJzAUUaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "axis_0 = np.concatenate([a, b], axis=0)\n",
        "print(\"axis 0\", axis_0, axis_0.shape)\n",
        "\n",
        "axis_1 = np.concatenate([a, b], axis=1)\n",
        "print(\"axis 1\", axis_1, axis_1.shape)"
      ],
      "metadata": {
        "id": "RdDxcGOSUyJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a + 1)\n",
        "print(a*b.T)\n",
        "\n",
        "c = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(\"c\", c.shape)\n",
        "print(a+c)\n",
        "\n",
        "# equivalent:\n",
        "a_expanded = np.tile(a, (3, 1))\n",
        "print(a_expanded, a_expanded.shape)\n",
        "print(a_expanded + c)"
      ],
      "metadata": {
        "id": "2N73JzEbU80W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tensorflow and Keras framework\n",
        "\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "\n",
        "\n",
        "TensorFlow and Keras are popular open-source machine learning frameworks widely used in building and training deep learning models.\n",
        "\n",
        "TensorFlow is an open-source platform for building machine learning and deep learning models. It provides a set of libraries for data manipulation, preprocessing, and visualization, along with a high-level API to build and train deep neural networks. TensorFlow is highly scalable and can be used to train models on a single machine or distributed across multiple servers or GPUs.\n",
        "\n",
        "Keras, on the other hand, is a high-level API for building and training deep learning models. It is built on top of TensorFlow and provides a user-friendly interface for building complex deep learning models quickly. Keras abstracts away many of the low-level details of TensorFlow, making it easier for beginners to get started with deep learning. Keras includes a wide range of pre-built layers and models, making it easier to construct complex neural networks.\n",
        "\n",
        "TensorFlow and Keras are important because they simplify the development and training of deep learning models, which can be complex and time-consuming. They provide a high-level API for building neural networks, allowing developers to focus on the model's architecture and not on the implementation details. TensorFlow and Keras also provide tools for model visualization, making it easier to understand the model's performance and make improvements.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xRs_UfdYZnfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup environment\n",
        "\n",
        "Go to Runtime -> Change runtime type -> Choose GPU"
      ],
      "metadata": {
        "id": "EQeC5IOTGue3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "Tensorflow offers many built in datasets. We will use MNIST which is composed of grayscale images that depict digits, and it can be loaded as follows:"
      ],
      "metadata": {
        "id": "n-jhSPU0djaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = data.load_data()"
      ],
      "metadata": {
        "id": "MVvINqfhdmTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.random.randint(0, len(X_train))\n",
        "plt.imshow(X_train[idx], cmap='gray')\n",
        "plt.title(f\"Label: {Y_train[idx]}\")"
      ],
      "metadata": {
        "id": "HYxG9nCa7guX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, custom datasets can be defined.\n",
        "\n",
        "The next step is to preprocess the data:"
      ],
      "metadata": {
        "id": "12auK_Io3J0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(X):\n",
        "    X = K.utils.normalize(X)\n",
        "    X = tf.expand_dims(X, axis=-1)\n",
        "    return X"
      ],
      "metadata": {
        "id": "e26k_7GT3LrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the function above, X represents the tensor with the images, and the first step is to normalize the image and then an extra dimension (channel dimension) is added. The function is called as follows:"
      ],
      "metadata": {
        "id": "igoz1pQR3MxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = process_data(X_train)\n",
        "X_test = process_data(X_test)\n",
        "\n",
        "print(\"Training data shape: \", X_train.shape)\n",
        "print(\"Test data shape: \", X_test.shape)"
      ],
      "metadata": {
        "id": "sD8svte83Q-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normalization is optional, but it helps the learning, and adding an extra channel dimension is necessary when using convolutions.\n",
        "\n",
        "# Model\n",
        "\n",
        "The next step is defining the model. The functional way of defining it in our case is the following:"
      ],
      "metadata": {
        "id": "WPKa95yq3S-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createModel(input_shape, num_classes, optimizer, loss, metrics):\n",
        "    inputs = K.layers.Input(input_shape)\n",
        "    x = K.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(inputs)\n",
        "    x = K.layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation='relu')(x)\n",
        "    x = K.layers.MaxPool2D()(x)\n",
        "    x = K.layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation='relu')(x)\n",
        "    x = K.layers.MaxPool2D()(x)\n",
        "    x = K.layers.Flatten()(x)\n",
        "    x = K.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = K.models.Model(inputs=inputs, outputs=x)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "    return model\n",
        "\n",
        "model = createModel(input_shape=(28, 28, 1), num_classes=10, optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LWYSbh_y3bNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several layers used, such as Conv2d, MaxPool2d or Dense, and Keras offers many more, depending on the problem.\n",
        "\n",
        "The first stage is defining the structure of the model, then it needs to be compiled with the optimizer, loss and metrics. In our case, the loss is sparse because the labels are provided as integers.\n",
        "\n",
        "The exact structure can be seen ass follows:"
      ],
      "metadata": {
        "id": "anZBIsuZ9W52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "FWLmbCOO9afo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is useful because we can see the exact number of parameters for each layer.\n",
        "\n",
        "# Training\n",
        "\n",
        "The training process is quite straightforward, because tensorflow hides the complex mathematical computations behind the scenes. It is enough to use the following:"
      ],
      "metadata": {
        "id": "arWzO2Zd9biO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, epochs=3, batch_size=256, validation_split=0.2)"
      ],
      "metadata": {
        "id": "VK1yZQHk_i2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And in order to save the model after training:"
      ],
      "metadata": {
        "id": "z3n-KzB1_krj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model\")"
      ],
      "metadata": {
        "id": "kSFiC3Q69iSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afterwards, the model can be loaded as such:"
      ],
      "metadata": {
        "id": "kz78OjBa9j7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = K.models.load_model('model')"
      ],
      "metadata": {
        "id": "Kla0wNMy9lOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "After training, it might be desirable to see the performance on a separate part of the dataset, which has not been used in training, and it is usually called \"test set\" and it can be done using one line:"
      ],
      "metadata": {
        "id": "iLuFatan9nI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = K.models.load_model('model')\n",
        "model.compile(metrics='accuracy')\n",
        "\n",
        "results = model.evaluate(X_test, Y_test, batch_size=256)"
      ],
      "metadata": {
        "id": "lmXevbBW9qsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n",
        "Another important aspect is the inference. This basically consists of giving the model some specific inputs and see the results. For instance, we can generate some random indexes and select the data at those indexes:"
      ],
      "metadata": {
        "id": "IG_cI2w293U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = np.random.randint(0, len(X_test), size=(9,))\n",
        "\n",
        "X_test_sample = tf.gather(X_test, idxs) # tf equivalent of X_test[idxs]\n",
        "Y_test_sample = tf.gather(Y_test, idxs)"
      ],
      "metadata": {
        "id": "Cj4jgSfF9-Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afterwards we can get the predicted probabilities out of which the digits can be extracted:"
      ],
      "metadata": {
        "id": "lCeQgk5E-GEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_probabilities = model(X_test_sample)\n",
        "predicted_digits = tf.argmax(predicted_probabilities, axis=-1)\n",
        "\n",
        "print(\"Predicted: \", predicted_digits)\n",
        "print(\"Real:      \", Y_test_sample)\n",
        "\n",
        "for i, (img, pred, real) in enumerate(zip(X_test_sample, predicted_digits, Y_test_sample), start=1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    plt.imshow(img.numpy(), cmap='gray')\n",
        "    plt.title(f\"Pred: {pred.numpy()} Real: {real.numpy()}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xeHQ-IjV-Hr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}